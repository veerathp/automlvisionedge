{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# AutoML training image image classification model for export to edge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl,export_edge"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use the Vertex AI SDK to create image classification models to export as an Edge model using AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,export_edge"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you create an AutoML classification model from a Python script using the Vertex SDK, and then export the model as an Edge model in TFLite format. You can alternatively create models with AutoML using the `gcloud` command-line tool or online using the Cloud Console.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- Vertex AI `Datasets`\n",
    "- AutoML Image\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex `Dataset` resource.\n",
    "- Train the model.\n",
    "- Export the `Edge` model from the `Model` resource to Cloud Storage.\n",
    "- Download the model locally.\n",
    "- Make a local prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:salads,iod"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is a visual inspection auto dataset containing defective and non-defective items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex AI SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.23.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.24.0-py2.py3-none-any.whl (2.5 MB)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.10.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.7.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.22.2)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Using cached protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.16.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Using cached google_cloud_resource_manager-1.9.1-py2.py3-none-any.whl (276 kB)\n",
      "  Using cached google_cloud_resource_manager-1.8.1-py2.py3-none-any.whl (235 kB)\n",
      "  Using cached google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n",
      "  Using cached google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
      "  Using cached google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n",
      "Installing collected packages: protobuf, packaging, google-cloud-resource-manager, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'service.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"pv-mlops-demo\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvQeFm3Gv5mR"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad1138a125ea"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ce6043da7b33"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13224697bfb"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://automl-vision-workshop-{PROJECT_ID}\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://automl-vision-workshop-pv-mlops-demo/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "# Tutorial\n",
    "\n",
    "Now you are ready to start creating your own AutoML image object detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "#### Location of Cloud Storage training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the CSV index file in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "import_file:salads,csv,iod"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://pv-visual-inspection-ai-auto/visual_inspection/parts_label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:csv"
   },
   "source": [
    "#### Quick peek at your data\n",
    "\n",
    "This tutorial uses a version of the Salads dataset that is stored in a public Cloud Storage bucket, using a CSV index file.\n",
    "\n",
    "Start by doing a quick peek at the data. You count the number of examples by counting the number of rows in the CSV index file  (`wc -l`) and then peek at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "quick_peek:csv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples 29\n",
      "First 10 rows\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_1.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_2.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_3.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_4.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_5.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_6.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_7.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_8.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_9.jpeg,defective\n",
      "gs://pv-visual-inspection-ai-auto/visual_inspection/defective/defect_10.jpeg,defective\n"
     ]
    }
   ],
   "source": [
    "if \"IMPORT_FILES\" in globals():\n",
    "    FILE = IMPORT_FILES[0]\n",
    "else:\n",
    "    FILE = IMPORT_FILE\n",
    "\n",
    "count = ! gsutil cat $FILE | wc -l\n",
    "print(\"Number of Examples\", int(count[0]))\n",
    "\n",
    "print(\"First 10 rows\")\n",
    "! gsutil cat $FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:image,iod"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `ImageDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `import_schema_uri`: The data labeling schema for the data items.\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "create_dataset:image,iod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/999012933750/locations/us-central1/datasets/2038958551803625472/operations/7100144508716711936\n",
      "ImageDataset created. Resource name: projects/999012933750/locations/us-central1/datasets/2038958551803625472\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/999012933750/locations/us-central1/datasets/2038958551803625472')\n",
      "Importing ImageDataset data: projects/999012933750/locations/us-central1/datasets/2038958551803625472\n",
      "Import ImageDataset data backing LRO: projects/999012933750/locations/us-central1/datasets/2038958551803625472/operations/4570951105483112448\n",
      "ImageDataset data imported. Resource name: projects/999012933750/locations/us-central1/datasets/2038958551803625472\n",
      "projects/999012933750/locations/us-central1/datasets/2038958551803625472\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"visualinspectionauto\",\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:image,edge,iod"
   },
   "source": [
    "### Create and run training pipeline\n",
    "\n",
    "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
    "\n",
    "#### Create training pipeline\n",
    "\n",
    "An AutoML training pipeline is created with the `AutoMLImageTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `prediction_type`: The type task to train the model for.\n",
    "  - `classification`: An image classification model.\n",
    "  - `object_detection`: An image object detection model.\n",
    "- `multi_label`: If a classification task, whether single (`False`) or multi-labeled (`True`).\n",
    "- `model_type`: The type of model for deployment.\n",
    "  - `CLOUD`: Deployment on Google Cloud\n",
    "  - `CLOUD_HIGH_ACCURACY_1`: Optimized for accuracy over latency for deployment on Google Cloud.\n",
    "  - `CLOUD_LOW_LATENCY_`: Optimized for latency over accuracy for deployment on Google Cloud.\n",
    "  - `MOBILE_TF_VERSATILE_1`: Deployment on an edge device.\n",
    "  - `MOBILE_TF_HIGH_ACCURACY_1`:Optimized for accuracy over latency for deployment on an edge device.\n",
    "  - `MOBILE_TF_LOW_LATENCY_1`: Optimized for latency over accuracy for deployment on an edge device.\n",
    "- `base_model`: (optional) Transfer learning from existing `Model` resource -- supported for image classification only.\n",
    "\n",
    "The instantiated object is the DAG (directed acyclic graph) for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "create_automl_pipeline:image,edge,iod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.AutoMLImageTrainingJob object at 0x7fed505fac10>\n"
     ]
    }
   ],
   "source": [
    "dag = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=\"visualinspectionauto\",\n",
    "    prediction_type=\"classification\",\n",
    "    multi_label=False,\n",
    "    model_type=\"MOBILE_TF_LOW_LATENCY_1\",\n",
    "    base_model=None,\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:image"
   },
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you run the DAG to start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
    "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
    "- `disable_early_stopping`: If `True`, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take around 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "run_automl_pipeline:image"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/625206913126105088?project=999012933750\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLImageTrainingJob run completed. Resource name: projects/999012933750/locations/us-central1/trainingPipelines/625206913126105088\n",
      "Model available at projects/999012933750/locations/us-central1/models/6723955607524605952\n"
     ]
    }
   ],
   "source": [
    "model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"visualinspectionauto\",\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    budget_milli_node_hours=1000,\n",
    "    disable_early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "source": [
    "## Review model evaluation scores\n",
    "\n",
    "After your model training has finished, you can review the evaluation scores for it using the `list_model_evaluations()` method. This method will return an iterator for each evaluation slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'projects/999012933750/locations/us-central1/models/6723955607524605952@1/evaluations/8326995663346728960', 'metricsSchemaUri': 'gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml', 'metrics': {'logLoss': 0.098869264, 'auPrc': 1.0, 'confidenceMetrics': [{'precision': 0.5, 'recall': 1.0}, {'precision': 0.5, 'confidenceThreshold': 0.05, 'recall': 1.0}, {'precision': 0.6666667, 'confidenceThreshold': 0.1, 'recall': 1.0}, {'recall': 1.0, 'precision': 1.0, 'confidenceThreshold': 0.15}, {'confidenceThreshold': 0.2, 'recall': 1.0, 'precision': 1.0}, {'confidenceThreshold': 0.25, 'precision': 1.0, 'recall': 1.0}, {'precision': 1.0, 'confidenceThreshold': 0.3, 'recall': 1.0}, {'precision': 1.0, 'recall': 1.0, 'confidenceThreshold': 0.35}, {'precision': 1.0, 'confidenceThreshold': 0.4, 'recall': 1.0}, {'confidenceThreshold': 0.45, 'precision': 1.0, 'recall': 1.0}, {'recall': 1.0, 'precision': 1.0, 'confidenceThreshold': 0.5}, {'confidenceThreshold': 0.55, 'precision': 1.0, 'recall': 1.0}, {'recall': 1.0, 'precision': 1.0, 'confidenceThreshold': 0.6}, {'confidenceThreshold': 0.65, 'precision': 1.0, 'recall': 1.0}, {'confidenceThreshold': 0.7, 'precision': 1.0, 'recall': 1.0}, {'recall': 1.0, 'confidenceThreshold': 0.75, 'precision': 1.0}, {'confidenceThreshold': 0.8, 'precision': 1.0, 'recall': 1.0}, {'confidenceThreshold': 0.85, 'precision': 1.0, 'recall': 1.0}, {'confidenceThreshold': 0.875, 'recall': 1.0, 'precision': 1.0}, {'recall': 1.0, 'confidenceThreshold': 0.9, 'precision': 1.0}, {'recall': 0.5, 'precision': 1.0, 'confidenceThreshold': 0.91}, {'precision': 1.0, 'confidenceThreshold': 0.92}, {'confidenceThreshold': 0.93, 'precision': 1.0}, {'confidenceThreshold': 0.94, 'precision': 1.0}, {'confidenceThreshold': 0.95, 'precision': 1.0}, {'confidenceThreshold': 0.96, 'precision': 1.0}, {'confidenceThreshold': 0.97, 'precision': 1.0}, {'precision': 1.0, 'confidenceThreshold': 0.98}, {'precision': 1.0, 'confidenceThreshold': 0.99}, {'precision': 1.0, 'confidenceThreshold': 0.995}, {'precision': 1.0, 'confidenceThreshold': 0.996}, {'precision': 1.0, 'confidenceThreshold': 0.997}, {'confidenceThreshold': 0.998, 'precision': 1.0}, {'confidenceThreshold': 0.999, 'precision': 1.0}, {'confidenceThreshold': 1.0, 'precision': 1.0}], 'confusionMatrix': {'annotationSpecs': [{'displayName': 'defective', 'id': '1444726942816272384'}, {'id': '6056412961243660288', 'displayName': 'ok'}], 'rows': [[1.0, 0.0], [0.0, 1.0]]}}, 'createTime': '2023-04-19T06:02:06.263929Z', 'sliceDimensions': ['annotationSpec'], 'annotationSchemaUri': 'gs://google-cloud-aiplatform/schema/dataset/annotation/image_classification_1.0.0.yaml'}\n"
     ]
    }
   ],
   "source": [
    "model_evaluations = model.list_model_evaluations()\n",
    "\n",
    "for model_evaluation in model_evaluations:\n",
    "    print(model_evaluation.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_model:mbsdk,image"
   },
   "source": [
    "## Export as Edge model\n",
    "\n",
    "You can export an AutoML image object detection model as a `Edge` model which you can then custom deploy to an edge device or download locally. Use the method `export_model()` to export the model to Cloud Storage, which takes the following parameters:\n",
    "\n",
    "- `artifact_destination`: The Cloud Storage location to store the SavedFormat model artifacts to.\n",
    "- `export_format_id`: The format to save the model format as. For AutoML image object detection there is just one option:\n",
    "   - `tf-saved-model`: TensorFlow SavedFormat for deployment to a container.\n",
    "   - `tflite`: TensorFlow Lite for deployment to an edge or mobile device.\n",
    "   - `edgetpu-tflite`: TensorFlow Lite for TPU\n",
    "   - `tf-js`: TensorFlow for web client\n",
    "   - `coral-ml`: for Coral devices\n",
    "\n",
    "- `sync`: Whether to perform operational sychronously or asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "export_model:mbsdk,image"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Model model: projects/999012933750/locations/us-central1/models/6723955607524605952\n",
      "Export Model model backing LRO: projects/999012933750/locations/us-central1/models/6723955607524605952/operations/2822850762621583360\n",
      "Model model exported. Resource name: projects/999012933750/locations/us-central1/models/6723955607524605952\n"
     ]
    }
   ],
   "source": [
    "response = model.export_model(\n",
    "    artifact_destination=BUCKET_URI, export_format_id=\"tflite\", sync=True\n",
    ")\n",
    "\n",
    "model_package = response[\"artifactOutputUri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_model_artifacts:tflite"
   },
   "source": [
    "#### Download the TFLite model artifacts\n",
    "\n",
    "Now that you have an exported TFLite version of your model, you can test the exported model locally, but first downloading it from Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "download_model_artifacts:tflite"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://automl-vision-workshop-pv-mlops-demo/model-6723955607524605952/tflite/2023-04-19T06:20:59.181313Z/model.tflite\n",
      "Copying gs://automl-vision-workshop-pv-mlops-demo/model-6723955607524605952/tflite/2023-04-19T06:20:59.181313Z/model.tflite...\n",
      "/ [1 files][549.9 KiB/549.9 KiB]                                                \n",
      "Operation completed over 1 objects/549.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $model_package\n",
    "# Download the model artifacts\n",
    "! gsutil cp -r $model_package tflite\n",
    "\n",
    "tflite_path = \"tflite/model.tflite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instantiate_tflite_interpreter"
   },
   "source": [
    "#### Instantiate a TFLite interpreter\n",
    "\n",
    "The TFLite version of the model is not a TensorFlow SavedModel format. You cannot directly use methods like predict(). Instead, one uses the TFLite interpreter. You must first setup the interpreter for the TFLite model as follows:\n",
    "\n",
    "- Instantiate an TFLite interpreter for the TFLite model.\n",
    "- Instruct the interpreter to allocate input and output tensors for the model.\n",
    "- Get detail information about the models input and output tensors that will need to be known for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "instantiate_tflite_interpreter"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 06:21:26.775022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 06:21:27.101958: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-19 06:21:31.061568: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-19 06:21:31.061843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-19 06:21:31.061862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape [  1 224 224   3]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0][\"shape\"]\n",
    "\n",
    "print(\"input tensor shape\", input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item"
   },
   "source": [
    "### Get test item\n",
    "\n",
    "You will use an arbitrary example out of the dataset as a test item. Don't be concerned that the example was likely used in training the model -- we just want to demonstrate how to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "get_test_item:image,224x224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image shape (419, 448, 3)\n",
      "test image shape (224, 224, 3) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "test_items = ! gsutil cat $IMPORT_FILE | head -n16\n",
    "test_item = test_items[0].split(\",\")[0]  #defective\n",
    "#test_item = test_items[15].split(\",\")[0] #ok\n",
    "with tf.io.gfile.GFile(test_item, \"rb\") as f:\n",
    "    content = f.read()\n",
    "test_image = tf.io.decode_jpeg(content)\n",
    "print(\"test image shape\", test_image.shape)\n",
    "\n",
    "test_image = tf.image.resize(test_image, (224, 224))\n",
    "print(\"test image shape\", test_image.shape, test_image.dtype)\n",
    "\n",
    "test_image = tf.cast(test_image, dtype=tf.uint8).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "invoke_tflite_interpreter"
   },
   "source": [
    "#### Make a prediction with TFLite model\n",
    "\n",
    "Finally, you do a prediction using your TFLite model, as follows:\n",
    "\n",
    "- Convert the test image into a batch of a single image (`np.expand_dims`)\n",
    "- Set the input tensor for the interpreter to your batch of a single image (`data`).\n",
    "- Invoke the interpreter.\n",
    "- Retrieve the softmax probabilities for the prediction (`get_tensor`).\n",
    "- Determine which label had the highest probability (`np.argmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "invoke_tflite_interpreter"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "interpreter.set_tensor(input_details[0][\"index\"], data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "softmax = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "label = np.argmax(softmax)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "# delete_bucket = False\n",
    "\n",
    "# # Delete the dataset using the Vertex dataset object\n",
    "# dataset.delete()\n",
    "\n",
    "# # Delete the model using the Vertex model object\n",
    "# model.delete()\n",
    "\n",
    "# # Delete the AutoML trainig job\n",
    "# dag.delete()\n",
    "\n",
    "# if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "#     ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "automl_image_object_detection_export_edge.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
